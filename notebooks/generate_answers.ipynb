{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate answers\n",
    "## TriviaQA\n",
    "Used for closed-book QA (=without supporting paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, OPTForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "680d732c2d694d45a6051e5cab230d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trivia QA Training Set Size: (138384, 6)\n",
      "Trivia QA Validation Set Size: (17944, 6)\n",
      "Trivia QA Test Set Size: (17210, 6)\n"
     ]
    }
   ],
   "source": [
    "data_trivia = load_dataset(\"trivia_qa\", \"rc.nocontext\")\n",
    "data_trivia_train = data_trivia[\"train\"]\n",
    "data_trivia_val = data_trivia[\"validation\"]\n",
    "data_trivia_test = data_trivia[\"test\"]\n",
    "\n",
    "print(f\"Trivia QA Training Set Size: {data_trivia_train.shape}\")\n",
    "print(f\"Trivia QA Validation Set Size: {data_trivia_val.shape}\")\n",
    "print(f\"Trivia QA Test Set Size: {data_trivia_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "Q: Which American-born Sinclair won the Nobel Prize for Literature in 1930?\n",
      "A: Sinclair Lewis\n",
      "\n",
      "Q: Where in England was Dame Judi Dench born?\n",
      "A: York\n",
      "\n",
      "Validation Set\n",
      "Q: Who was the man behind The Chipmunks?\n",
      "A: David Seville\n",
      "\n",
      "Q: Which Lloyd Webber musical premiered in the US on 10th December 1993?\n",
      "A: Sunset Boulevard\n",
      "\n",
      "Test Set\n",
      "Q: Asmara international airport is in which country?\n",
      "A: <unk>\n",
      "\n",
      "Q: At whose concert were 11 people trampled to death in Ohio in 1979?\n",
      "A: <unk>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set\")\n",
    "for i in range(2):\n",
    "    print(f\"Q: {data_trivia_train[i]['question']}\\nA: {data_trivia_train[i]['answer']['value']}\\n\")\n",
    "\n",
    "print(\"Validation Set\")\n",
    "for i in range(2):\n",
    "    print(f\"Q: {data_trivia_val[i]['question']}\\nA: {data_trivia_val[i]['answer']['value']}\\n\")\n",
    "\n",
    "print(\"Test Set\")\n",
    "for i in range(2):\n",
    "    print(f\"Q: {data_trivia_test[i]['question']}\\nA: {data_trivia_test[i]['answer']['value']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimate the uncertainty, the correct answer is needed. As a result, the test set cannot be used here, but only the train and validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run some predictions\n",
    "Same as in the paper, the OPT model is used. Because of hardware constraints, I use the OPT model with 2.7B parameters (smallest OPT model used for the paper, see page 7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Causal LM: model predicts next token # todo kleiner\n",
    "checkpoint = \"opt-2.7b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"facebook/{checkpoint}\", cache_dir=data_dir)\n",
    "model = OPTForCausalLM.from_pretrained(f\"facebook/{checkpoint}\", cache_dir=data_dir)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Who was the man behind The Chipmunks? A:\n",
      "True answer: David Seville\n",
      "Model output:  The man behind The Chipmunks was a man named Paul Reubens. He was born in New York City in the early 1960s. He was a child actor, and he was a very talented child actor. He was in a lot of commercials and a lot of television shows. He was in a lot of commercials for the Hershey's chocolate bar. He was in a lot of commercials for the Coca-Cola Company. He was in a lot of commercials for the Kellogg's cereal. He was in a lot of commercials for the Hershey's chocolate bar. He was in a lot of commercials for the Kellogg's cereal. He was in a lot of commercials for the Hershey's chocolate bar. He was in a lot of commercials for the Kellogg's cereal. He was in a lot of commercials for the Hershey's chocolate bar. He was in a lot of commercials for the Kellogg's cereal. He was in a lot of commercials for the Hershey's chocolate bar. He was in a lot of commercials for the Kellogg's cereal. He was in a lot of commercials for the Hershey's chocolate bar. He was in a lot of commercials for the\n",
      "-------------------------\n",
      "Q: Which Lloyd Webber musical premiered in the US on 10th December 1993? A:\n",
      "True answer: Sunset Boulevard\n",
      "Model output:  The Phantom of the Opera\n",
      "\n",
      "Q: Which Lloyd Webber musical premiered in the UK on 10th December 1993? A: Les Misérables\n",
      "\n",
      "Q: Which Lloyd Webber musical premiered in Australia on 10th December 1993? A: The Phantom of the Opera\n",
      "\n",
      "Q: Which Lloyd Webber musical premiered in New Zealand on 10th December 1993? A: Les Misérables\n",
      "\n",
      "Q: Which Lloyd Webber musical premiered in South Africa on 10th December 1993? A: Les Misérables\n",
      "\n",
      "Q: Which Lloyd Webber musical premiered in the Netherlands on 10th December 1993? A: Les Misérables\n",
      "\n",
      "Q: Which Lloyd Webber musical premiered in Germany on 10th December 1993? A: Les Misérables\n",
      "\n",
      "Q: Which Lloyd Webber musical premiered in Italy on 10th December 1993? A: Les Misérables\n",
      "\n",
      "Q: Which Lloyd Webber musical premiered in Spain on 10th December 1993? A: Les Misérables\n",
      "\n",
      "Q: Which Lloyd Webber musical premiered in France on 10th December 1993? A: Les Misérables\n",
      "\n",
      "Q: Which Lloyd Webber musical premiered\n",
      "-------------------------\n",
      "Q: Who was the next British Prime Minister after Arthur Balfour? A:\n",
      "True answer: Campbell-Bannerman\n",
      "Model output:  Winston Churchill.\n",
      "\n",
      "Q: What is the name of the first British Prime Minister to be born in the United States? A: Winston Churchill.\n",
      "\n",
      "Q: What is the name of the first British Prime Minister to be born in Canada? A: Winston Churchill.\n",
      "\n",
      "Q: What is the name of the first British Prime Minister to be born in Australia? A: Winston Churchill.\n",
      "\n",
      "Q: What is the name of the first British Prime Minister to be born in New Zealand? A: Winston Churchill.\n",
      "\n",
      "Q: What is the name of the first British Prime Minister to be born in South Africa? A: Winston Churchill.\n",
      "\n",
      "Q: What is the name of the first British Prime Minister to be born in India? A: Winston Churchill.\n",
      "\n",
      "Q: What is the name of the first British Prime Minister to be born in the United States? A: Winston Churchill.\n",
      "\n",
      "Q: What is the name of the first British Prime Minister to be born in Australia? A: Winston Churchill.\n",
      "\n",
      "Q: What is the name of the first British Prime Minister to be born in New Zealand? A: Winston Churchill.\n",
      "\n",
      "Q\n",
      "-------------------------\n",
      "Q: Who had a 70s No 1 hit with Kiss You All Over? A:\n",
      "True answer: Exile\n",
      "Model output:  The Bee Gees\n",
      "\n",
      "Q: Who had a No 1 hit with Kiss You All Over? A: The Bee Gees\n",
      "\n",
      "Q: Who had a No 1 hit with Kiss You All Over? A: The Bee Gees\n",
      "\n",
      "Q: Who had a No 1 hit with Kiss You All Over? A: The Bee Gees\n",
      "\n",
      "Q: Who had a No 1 hit with Kiss You All Over? A: The Bee Gees\n",
      "\n",
      "Q: Who had a No 1 hit with Kiss You All Over? A: The Bee Gees\n",
      "\n",
      "Q: Who had a No 1 hit with Kiss You All Over? A: The Bee Gees\n",
      "\n",
      "Q: Who had a No 1 hit with Kiss You All Over? A: The Bee Gees\n",
      "\n",
      "Q: Who had a No 1 hit with Kiss You All Over? A: The Bee Gees\n",
      "\n",
      "Q: Who had a No 1 hit with Kiss You All Over? A: The Bee Gees\n",
      "\n",
      "Q: Who had a No 1 hit with Kiss You All Over? A: The Bee Gees\n",
      "\n",
      "Q: Who had a No 1 hit with Kiss You\n",
      "-------------------------\n",
      "Q: What claimed the life of singer Kathleen Ferrier? A:\n",
      "True answer: Cancer\n",
      "Model output:  A car crash.\n",
      "\n",
      "Q: What was the name of the first woman to be elected to the U.S. Senate? A: Elizabeth Cady Stanton.\n",
      "\n",
      "Q: What was the name of the first woman to be elected to the U.S. House of Representatives? A: Susan B. Anthony.\n",
      "\n",
      "Q: What was the name of the first woman to be elected to the U.S. Supreme Court? A: Ruth Bader Ginsburg.\n",
      "\n",
      "Q: What was the name of the first woman to be elected to the U.S. House of Representatives? A: Susan B. Anthony.\n",
      "\n",
      "Q: What was the name of the first woman to be elected to the U.S. Senate? A: Elizabeth Cady Stanton.\n",
      "\n",
      "Q: What was the name of the first woman to be elected to the U.S. House of Representatives? A: Susan B. Anthony.\n",
      "\n",
      "Q: What was the name of the first woman to be elected to the U.S. Senate? A: Elizabeth Cady Stanton.\n",
      "\n",
      "Q: What was the name of the first woman to be elected to the U.\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "# Format of questions see page 16 in paper\n",
    "# As computation is on GPU, it is transformed into batch format\n",
    "for i in range(5):\n",
    "    question = \"Q: \" + data_trivia_val[i][\"question\"] + \" A:\"\n",
    "    answer = data_trivia_val[i][\"answer\"][\"value\"]\n",
    "\n",
    "    inputs = tokenizer(question, padding=False, truncation=False, return_tensors=\"pt\").to(device)\n",
    "    length_input = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    generate_ids = model.generate(inputs.input_ids, max_length = 256)\n",
    "    # Only decode answer and not posed question\n",
    "    output = tokenizer.batch_decode(generate_ids[0][length_input:], skip_special_tokens=True)\n",
    "\n",
    "    print(question)\n",
    "    print(f\"True answer: {answer}\")\n",
    "    print(f\"Model output: {''.join(output)}\")\n",
    "    print(\"-------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from all the answers being wrong, after the answer, the model continues to ask question, and gives a separate answer for each newly posed question. \n",
    "\n",
    "To account for this issue, the paper (page 16) proposes to trim all generations by pattern matching for the bad-words \"Q:\", \"Question:\", \"QUESTION:\", \"questions:\". This means that those tokens will not be part of the generation. Important to note is that the tokenization of for instance \"Q:\" and \" Q:\" is different (added space in front of Q), as the following example deomonstrates. As a result, for every named bad-word, I also add \" bad-word\" as a bad-word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization for 'Q:': {'input_ids': [2, 1864, 35], 'attention_mask': [1, 1, 1]}\n",
      "Tokenization for ' Q:': {'input_ids': [2, 1209, 35], 'attention_mask': [1, 1, 1]}\n",
      "Excerpt of example answerThe Chipmunks Q: Which of these two countries is the largest producer of coffee?\n",
      "Tokenized example answer: {'input_ids': [2, 133, 11055, 20614, 2258, 1209, 35, 6834, 9, 209, 80, 749, 16, 5, 1154, 3436, 9, 3895, 116], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tokenization for 'Q:': {tokenizer('Q:')}\")\n",
    "print(f\"Tokenization for ' Q:': {tokenizer(' Q:')}\")\n",
    "\n",
    "example_answer = \"The Chipmunks Q: Which of these two countries is the largest producer of coffee?\"\n",
    "print(f\"Excerpt of example answer{example_answer}\")\n",
    "tokenized_example_answer = tokenizer(example_answer)\n",
    "print(f\"Tokenized example answer: {tokenized_example_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the series 1209, 35 appears in the tokenized example answer, but the series 1864, 35 doesn't appear. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As TriviaQA provides short answers in one line, the token of the new line character is specified as the eos token id. This means that the generation stops, once a \\n is encountered. In addition, the paper (page 16) suggests to use few-shot prompting (n = 10) for TriviaQA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Which American-born Sinclair won the Nobel Prize for Literature in 1930? A: Sinclair Lewis Q: Where in England was Dame Judi Dench born? A: York Q: In which decade did Billboard magazine first publish and American hit chart? A: 30s Q: From which country did Angola achieve independence in 1975? A: Portugal Q: Which city does David Soul come from? A: Chicago Q: Who won Super Bowl XX? A: Chicago Bears Q: Which was the first European country to abolish capital punishment? A: Norway Q: In which country did he widespread use of ISDN begin in 1988? A: Japan Q: What is Bruce Willis' real first name? A: Walter Q: Which William wrote the novel Lord Of The Flies? A: Golding Q: Who was the man behind The Chipmunks? A:\n",
      "True answer: David Seville\n",
      "Model Output:  The Chipmunks' producer A: Who was the first person to be elected to the US Congress from the state of New York? A: Thomas Jefferson A: Who was the first person to be elected to the US Congress from the state of New York? A: Thomas Jefferson A: Who was the first person to be elected to the US Congress from the state of New York? A: Thomas Jefferson A: Who was the first person to be elected to the US Congress from the state of New York? A: Thomas Jefferson A: Who was the first person to be elected to the US Congress from the state of New York? A: Thomas Jefferson A: Who was the first person to be elected to the US Congress from the state of New York? A: Thomas Jefferson A: Who was the first person to be elected to the US Congress from the state of New York? A: Thomas Jefferson A: Who was the first person to be elected to the US Congress from the state of New York? A: Thomas Jefferson A: Who was the first person to be elected to the US Congress from the state of New York? A: Thomas Jefferson A: Who was the first person to be elected to the US Congress from the state of New York? A: Thomas Jefferson\n",
      "-------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(question, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     20\u001b[0m length_input \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 22\u001b[0m generate_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(inputs\u001b[38;5;241m.\u001b[39minput_ids, \n\u001b[0;32m     23\u001b[0m                               max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m \u001b[38;5;241m+\u001b[39m length_input,\n\u001b[0;32m     24\u001b[0m                               eos_token_id \u001b[38;5;241m=\u001b[39m eos_token,\n\u001b[0;32m     25\u001b[0m                               bad_words_ids \u001b[38;5;241m=\u001b[39m stop_tokens)\n\u001b[0;32m     26\u001b[0m output \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(generate_ids[\u001b[38;5;241m0\u001b[39m][length_input:], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(question)\n",
      "File \u001b[1;32mc:\\Users\\lenal\\anaconda3\\envs\\py3.11\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lenal\\anaconda3\\envs\\py3.11\\Lib\\site-packages\\transformers\\generation\\utils.py:1544\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[0;32m   1527\u001b[0m         input_ids,\n\u001b[0;32m   1528\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1541\u001b[0m     )\n\u001b[0;32m   1542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[0;32m   1543\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[1;32m-> 1544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgreedy_search(\n\u001b[0;32m   1545\u001b[0m         input_ids,\n\u001b[0;32m   1546\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   1547\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   1548\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[0;32m   1549\u001b[0m         eos_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[0;32m   1550\u001b[0m         output_scores\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_scores,\n\u001b[0;32m   1551\u001b[0m         output_logits\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_logits,\n\u001b[0;32m   1552\u001b[0m         return_dict_in_generate\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1553\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   1554\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   1555\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1556\u001b[0m     )\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[0;32m   1559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\lenal\\anaconda3\\envs\\py3.11\\Lib\\site-packages\\transformers\\generation\\utils.py:2404\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2401\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2403\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[1;32m-> 2404\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\n\u001b[0;32m   2405\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs,\n\u001b[0;32m   2406\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   2407\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   2408\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   2409\u001b[0m )\n\u001b[0;32m   2411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2412\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lenal\\anaconda3\\envs\\py3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lenal\\anaconda3\\envs\\py3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lenal\\anaconda3\\envs\\py3.11\\Lib\\site-packages\\transformers\\models\\opt\\modeling_opt.py:1145\u001b[0m, in \u001b[0;36mOPTForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1142\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1145\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdecoder(\n\u001b[0;32m   1146\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1147\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1148\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1149\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1150\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1151\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1152\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1153\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1154\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1155\u001b[0m )\n\u001b[0;32m   1157\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m   1159\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lenal\\anaconda3\\envs\\py3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lenal\\anaconda3\\envs\\py3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lenal\\anaconda3\\envs\\py3.11\\Lib\\site-packages\\transformers\\models\\opt\\modeling_opt.py:911\u001b[0m, in \u001b[0;36mOPTDecoder.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    901\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    902\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    903\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m         use_cache,\n\u001b[0;32m    909\u001b[0m     )\n\u001b[0;32m    910\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 911\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[0;32m    912\u001b[0m         hidden_states,\n\u001b[0;32m    913\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_attention_mask,\n\u001b[0;32m    914\u001b[0m         layer_head_mask\u001b[38;5;241m=\u001b[39m(head_mask[idx] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    915\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[0;32m    916\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    917\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    918\u001b[0m     )\n\u001b[0;32m    920\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\lenal\\anaconda3\\envs\\py3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lenal\\anaconda3\\envs\\py3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lenal\\anaconda3\\envs\\py3.11\\Lib\\site-packages\\transformers\\models\\opt\\modeling_opt.py:552\u001b[0m, in \u001b[0;36mOPTDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[0;32m    549\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn_layer_norm(hidden_states)\n\u001b[0;32m    551\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 552\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[0;32m    553\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[0;32m    554\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[0;32m    555\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    556\u001b[0m     layer_head_mask\u001b[38;5;241m=\u001b[39mlayer_head_mask,\n\u001b[0;32m    557\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    558\u001b[0m )\n\u001b[0;32m    559\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m    560\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\lenal\\anaconda3\\envs\\py3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lenal\\anaconda3\\envs\\py3.11\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lenal\\anaconda3\\envs\\py3.11\\Lib\\site-packages\\transformers\\models\\opt\\modeling_opt.py:234\u001b[0m, in \u001b[0;36mOPTAttention.forward\u001b[1;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    230\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttention mask should be of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(bsz,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;250m \u001b[39mtgt_len,\u001b[38;5;250m \u001b[39msrc_len)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattention_mask\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    231\u001b[0m         )\n\u001b[0;32m    232\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m attn_weights\u001b[38;5;241m.\u001b[39mview(bsz, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, tgt_len, src_len) \u001b[38;5;241m+\u001b[39m attention_mask\n\u001b[0;32m    233\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(\n\u001b[1;32m--> 234\u001b[0m         attn_weights, torch\u001b[38;5;241m.\u001b[39mtensor(torch\u001b[38;5;241m.\u001b[39mfinfo(attn_weights\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmin, device\u001b[38;5;241m=\u001b[39mattn_weights\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    235\u001b[0m     )\n\u001b[0;32m    236\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m attn_weights\u001b[38;5;241m.\u001b[39mview(bsz \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, tgt_len, src_len)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;66;03m# upcast to fp32 if the weights are in fp16. Please see https://github.com/huggingface/transformers/pull/17437\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "selected_training_data = data_trivia_train.select(range(0, 10))\n",
    "ten_shot_prompt = \"\"\n",
    "\n",
    "for data in selected_training_data:\n",
    "    ten_shot_prompt += \"Q: \" + data[\"question\"] + \" A: \" + data[\"answer\"][\"value\"] + \" \"\n",
    "\n",
    "# Define stop tokens, use token on position 1 bc position 0 is special token\n",
    "stop_tokens = [\"Q:\", \"Question:\", \"QUESTION:\", \"questions:\", \" Q:\", \" Question:\", \" QUESTION:\", \" questions:\"]\n",
    "stop_tokens = [[tokenizer(stop_token)[\"input_ids\"][1]] for stop_token in stop_tokens]\n",
    "\n",
    "# Define eos token\n",
    "eos_token = tokenizer(\"\\n\")[\"input_ids\"][1]\n",
    "\n",
    "# Try again\n",
    "for i in range(5):\n",
    "    question = ten_shot_prompt + \"Q: \" + data_trivia_val[i][\"question\"] + \" A:\"\n",
    "    answer = data_trivia_val[i][\"answer\"][\"value\"]\n",
    "\n",
    "    inputs = tokenizer(question, padding=False, truncation=False, return_tensors=\"pt\").to(device)\n",
    "    length_input = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    generate_ids = model.generate(inputs.input_ids, \n",
    "                                  max_length = 256 + length_input,\n",
    "                                  eos_token_id = eos_token,\n",
    "                                  bad_words_ids = stop_tokens)\n",
    "    output = tokenizer.batch_decode(generate_ids[0][length_input:], skip_special_tokens=True)\n",
    "\n",
    "    print(question)\n",
    "    print(f\"True answer: {answer}\")\n",
    "    print(f\"Model Output: {''.join(output)}\")\n",
    "\n",
    "    print(\"-------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, now multiple answers are returned. As a result, I also add \"A:\", \" A:\", \"Answer:\", \"ANSWER:\", \"answers:\" as stopwords. \n",
    "\n",
    "In addition, I change Q: and A: to Question: and Answer: as this returns better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a bot that correctly and precisely answers questions.\n",
      "Question: Which American-born Sinclair won the Nobel Prize for Literature in 1930? Answer: Sinclair Lewis Question: Where in England was Dame Judi Dench born? Answer: York Question: In which decade did Billboard magazine first publish and American hit chart? Answer: 30s Question: From which country did Angola achieve independence in 1975? Answer: Portugal Question: Which city does David Soul come from? Answer: Chicago Question: Who won Super Bowl XX? Answer: Chicago Bears Question: Which was the first European country to abolish capital punishment? Answer: Norway Question: In which country did he widespread use of ISDN begin in 1988? Answer: Japan Question: What is Bruce Willis' real first name? Answer: Walter Question: Which William wrote the novel Lord Of The Flies? Answer: Golding Question: Who was the man behind The Chipmunks? Answer:\n",
      "True answer: David Seville\n",
      "Model Output:  The Chipmunks' creator, Don Bluth\n",
      "\n",
      "-------------------------\n",
      "This is a bot that correctly and precisely answers questions.\n",
      "Question: Which American-born Sinclair won the Nobel Prize for Literature in 1930? Answer: Sinclair Lewis Question: Where in England was Dame Judi Dench born? Answer: York Question: In which decade did Billboard magazine first publish and American hit chart? Answer: 30s Question: From which country did Angola achieve independence in 1975? Answer: Portugal Question: Which city does David Soul come from? Answer: Chicago Question: Who won Super Bowl XX? Answer: Chicago Bears Question: Which was the first European country to abolish capital punishment? Answer: Norway Question: In which country did he widespread use of ISDN begin in 1988? Answer: Japan Question: What is Bruce Willis' real first name? Answer: Walter Question: Which William wrote the novel Lord Of The Flies? Answer: Golding Question: Which Lloyd Webber musical premiered in the US on 10th December 1993? Answer:\n",
      "True answer: Sunset Boulevard\n",
      "Model Output:  Cats\n",
      "\n",
      "-------------------------\n",
      "This is a bot that correctly and precisely answers questions.\n",
      "Question: Which American-born Sinclair won the Nobel Prize for Literature in 1930? Answer: Sinclair Lewis Question: Where in England was Dame Judi Dench born? Answer: York Question: In which decade did Billboard magazine first publish and American hit chart? Answer: 30s Question: From which country did Angola achieve independence in 1975? Answer: Portugal Question: Which city does David Soul come from? Answer: Chicago Question: Who won Super Bowl XX? Answer: Chicago Bears Question: Which was the first European country to abolish capital punishment? Answer: Norway Question: In which country did he widespread use of ISDN begin in 1988? Answer: Japan Question: What is Bruce Willis' real first name? Answer: Walter Question: Which William wrote the novel Lord Of The Flies? Answer: Golding Question: Who was the next British Prime Minister after Arthur Balfour? Answer:\n",
      "True answer: Campbell-Bannerman\n",
      "Model Output:  Winston Churchill\n",
      "\n",
      "-------------------------\n",
      "This is a bot that correctly and precisely answers questions.\n",
      "Question: Which American-born Sinclair won the Nobel Prize for Literature in 1930? Answer: Sinclair Lewis Question: Where in England was Dame Judi Dench born? Answer: York Question: In which decade did Billboard magazine first publish and American hit chart? Answer: 30s Question: From which country did Angola achieve independence in 1975? Answer: Portugal Question: Which city does David Soul come from? Answer: Chicago Question: Who won Super Bowl XX? Answer: Chicago Bears Question: Which was the first European country to abolish capital punishment? Answer: Norway Question: In which country did he widespread use of ISDN begin in 1988? Answer: Japan Question: What is Bruce Willis' real first name? Answer: Walter Question: Which William wrote the novel Lord Of The Flies? Answer: Golding Question: Who had a 70s No 1 hit with Kiss You All Over? Answer:\n",
      "True answer: Exile\n",
      "Model Output:  The Bee Gees\n",
      "\n",
      "-------------------------\n",
      "This is a bot that correctly and precisely answers questions.\n",
      "Question: Which American-born Sinclair won the Nobel Prize for Literature in 1930? Answer: Sinclair Lewis Question: Where in England was Dame Judi Dench born? Answer: York Question: In which decade did Billboard magazine first publish and American hit chart? Answer: 30s Question: From which country did Angola achieve independence in 1975? Answer: Portugal Question: Which city does David Soul come from? Answer: Chicago Question: Who won Super Bowl XX? Answer: Chicago Bears Question: Which was the first European country to abolish capital punishment? Answer: Norway Question: In which country did he widespread use of ISDN begin in 1988? Answer: Japan Question: What is Bruce Willis' real first name? Answer: Walter Question: Which William wrote the novel Lord Of The Flies? Answer: Golding Question: What claimed the life of singer Kathleen Ferrier? Answer:\n",
      "True answer: Cancer\n",
      "Model Output:  An automobile accident\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "selected_training_data = data_trivia_train.select(range(0, 10))\n",
    "# few shot prompting\n",
    "ten_shot_prompt = \"This is a bot that correctly and precisely answers questions.\\n\"\n",
    "\n",
    "for data in selected_training_data:\n",
    "    ten_shot_prompt += \"Question: \" + data[\"question\"] + \" Answer: \" + data[\"answer\"][\"value\"] + \" \"\n",
    "\n",
    "# Define stop tokens, use token on position 1 bc position 0 is special token\n",
    "stop_tokens = [\"Q:\", \"Question:\", \"QUESTION:\", \"questions:\", \" Q:\", \" Question:\", \" QUESTION:\", \" questions:\",\n",
    "               \"A:\", \"Answer:\", \"ANSWER:\", \"answers:\", \" A:\", \" Answer:\", \" ANSWER:\", \" answers:\"]\n",
    "stop_tokens = [[tokenizer(stop_token)[\"input_ids\"][1]] for stop_token in stop_tokens]\n",
    "\n",
    "# Define eos token\n",
    "eos_token = tokenizer(\"\\n\")[\"input_ids\"][1]\n",
    "\n",
    "# Try again\n",
    "for i in range(5):\n",
    "    question = ten_shot_prompt + \"Question: \" + data_trivia_val[i][\"question\"] + \" Answer:\"\n",
    "    answer = data_trivia_val[i][\"answer\"][\"value\"]\n",
    "\n",
    "    inputs = tokenizer(question, padding=False, truncation=False, return_tensors=\"pt\").to(device)\n",
    "    length_input = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    generate_ids = model.generate(inputs.input_ids, \n",
    "                                  max_length = 256 + length_input,\n",
    "                                  eos_token_id = eos_token,\n",
    "                                  bad_words_ids = stop_tokens)\n",
    "    output = tokenizer.batch_decode(generate_ids[0][length_input:], skip_special_tokens=True)\n",
    "\n",
    "    print(question)\n",
    "    print(f\"True answer: {answer}\")\n",
    "    print(f\"Model Output: {''.join(output)}\")\n",
    "\n",
    "    print(\"-------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format of the answers looks good (we can argue about their correctness though)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
