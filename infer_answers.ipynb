{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Group semantically equivalent answers into bins for each question\n",
    "Model: Deberta (https://huggingface.co/sileod/deberta-v3-large-tasksource-nli)"
   ],
   "id": "7559db96c7f2ccd0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T19:36:34.258728Z",
     "start_time": "2024-06-13T19:36:31.589141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import yaml\n",
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)"
   ],
   "id": "d36e7253f9fd0793",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T16:34:41.482235Z",
     "start_time": "2024-06-13T16:34:41.470234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ],
   "id": "a2df06ce282e404b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T16:53:42.211931Z",
     "start_time": "2024-06-13T16:53:42.191901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_dir = config[\"model_dir\"]\n",
    "save_path = config[\"path_to_saved_generations\"]"
   ],
   "id": "5115d3a4b654d3d3",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T16:37:09.940768Z",
     "start_time": "2024-06-13T16:36:52.878825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"sileod/deberta-v3-large-tasksource-nli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=model_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, cache_dir=model_dir).to(device)"
   ],
   "id": "402f3ff802a143da",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 16,
   "source": [
    "def load_pickle_files(folder):\n",
    "    data_groups = []\n",
    "    pickle_files = glob.glob(f\"{folder}/group*.pkl\")\n",
    "    for pickle_file in pickle_files:\n",
    "        with open(pickle_file, \"rb\") as f:\n",
    "            data_groups.append(pickle.load(f))\n",
    "\n",
    "    return data_groups"
   ],
   "id": "6cffe20802782df5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Example\n",
    "Two answers are semantically equivalent if \n",
    "- \"Question: *question* Answer: *generated answer*\" $\\Rightarrow$ \"Question: *question* Answer: *true answer*\" **and** \n",
    "- \"Question: *question* Answer: *true answer*\" $\\Rightarrow$ \"Question: *question* Answer: *generated answer*\""
   ],
   "id": "6b6e79e288b06c55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many continents does the world have? Answer: There are seven continents. => Question: How many continents does the world have? Answer: Seven.\n",
      "{'entailment': 99.4, 'neutral': 0.6, 'contradiction': 0.0}\n",
      "\n",
      "Question: How many continents does the world have? Answer: Seven. => Question: How many continents does the world have? Answer: There are seven continents.\n",
      "{'entailment': 97.1, 'neutral': 2.9, 'contradiction': 0.0}\n"
     ]
    }
   ],
   "execution_count": 11,
   "source": [
    "question = \"Question: How many continents does the world have?\"\n",
    "sequence1 = \"Answer: There are seven continents.\"\n",
    "sequence2 = \"Answer: Seven.\"\n",
    "\n",
    "# Direction 1\n",
    "premise = question + \" \" + sequence1\n",
    "hypothesis = question + \" \" + sequence2\n",
    "print(premise + \" => \" + hypothesis)\n",
    "input = tokenizer(premise, hypothesis, return_tensors=\"pt\")\n",
    "output = model(input[\"input_ids\"].to(device))\n",
    "prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "print(prediction, end=\"\\n\\n\")\n",
    "\n",
    "# Direction 2\n",
    "premise = question + \" \" + sequence2\n",
    "hypothesis = question + \" \" + sequence1\n",
    "print(premise + \" => \" + hypothesis)\n",
    "input = tokenizer(premise, hypothesis, return_tensors=\"pt\")\n",
    "output = model(input[\"input_ids\"].to(device))\n",
    "prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "print(prediction)"
   ],
   "id": "1f564835b3b4a12c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Save lexical equivalence groups\n",
    "Used to calculate predictive entropy\n",
    "\n",
    "Structure to save them\n",
    "```python \n",
    "{ 1131: {\"question\": ..., \n",
    "         \"true_answer\": ..., \n",
    "         \"temperature_0.25\": {\"answers\": [...], \"probabilities\": [...], \"length_output\": [...], \"lexical_eq_groups\": [0, 1, 0, 2, 3, ...]},\n",
    "         \"temperature_0.5\": {\"answers\": [...], \"probabilities\": [...], \"length_output\": [...], \"lexical_eq_groups\": [...]},\n",
    "         \"temperature_1\": {\"answers\": [...], \"probabilities\": [...], \"length_output\": [...], \"lexical_eq_groups\": [...]},\n",
    "         \"temperature_1.5\": {\"answers\": [...], \"probabilities\": [...], \"length_output\": [...], \"lexical_eq_groups\": [...]},\n",
    "         \"beam_20\": {\"answers\": [...], \"probabilities\": [...], \"length_output\": [...], \"lexical_eq_groups\": [...]}\n",
    "        }, \n",
    "  4295: ...\n",
    "}\n",
    "```\n",
    "where the numbers in lexical_eq_groups stand for the group they belong to."
   ],
   "id": "d30d627554808fdf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T19:36:10.632320Z",
     "start_time": "2024-06-13T19:36:06.092188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = load_pickle_files(save_path)\n",
    "config_keys = [f\"temperature_{t}\" for t in config[\"temperatures\"]] + [f\"beam_{b}\" for b in config[\"n_beams\"]]"
   ],
   "id": "295e322e3bd6dd39",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T19:36:52.339418Z",
     "start_time": "2024-06-13T19:36:46.882366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for group_nr, group in enumerate(data):\n",
    "    for question_id in tqdm(group):\n",
    "        for k in config_keys:\n",
    "            answers = group[question_id][k][\"answers\"]\n",
    "            lexical_eq_groups = [-1] * len(answers)\n",
    "            group_count = 0\n",
    "\n",
    "            for i in range(len(answers)):\n",
    "                if lexical_eq_groups[i] == -1:\n",
    "                    lexical_eq_groups[i] = group_count\n",
    "                    for j in range(i + 1, len(answers)):\n",
    "                        if answers[i] == answers[j]:\n",
    "                            lexical_eq_groups[j] = group_count\n",
    "                    group_count += 1\n",
    "            group[question_id][k][\"lexical_eq_groups\"] = lexical_eq_groups\n",
    "\n",
    "    # Save result\n",
    "    with open(os.path.join(save_path, f\"group{group_nr}.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(group, f)"
   ],
   "id": "b8eb4caa60728d1f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 16643.20it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 22476.31it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 22723.87it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 22207.71it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 21737.55it/s]\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Bidirectional Entailment Clustering\n",
    "Used to calculate semantic entropy\n",
    "\n",
    "Structure to save them\n",
    "```python \n",
    "{ 1131: {\"question\": ..., \n",
    "         \"true_answer\": ..., \n",
    "         \"temperature_0.25\": {\"answers\": [...], \"probabilities\": [...], \"length_output\": [...], \"lexical_eq_groups\": [...], \"semantic_eq_groups\": [...]},\n",
    "         \"temperature_0.5\": {\"answers\": [...], \"probabilities\": [...], \"length_output\": [...], \"lexical_eq_groups\": [...], \"semantic_eq_groups\": [...]},\n",
    "         \"temperature_1\": {\"answers\": [...], \"probabilities\": [...], \"length_output\": [...], \"lexical_eq_groups\": [...], \"semantic_eq_groups\": [...]},\n",
    "         \"temperature_1.5\": {\"answers\": [...], \"probabilities\": [...], \"length_output\": [...], \"lexical_eq_groups\": [...], \"semantic_eq_groups\": [...]},\n",
    "         \"beam_20\": {\"answers\": [...], \"probabilities\": [...], \"length_output\": [...], \"lexical_eq_groups\": [...], \"semantic_eq_groups\": [...]}\n",
    "        }, \n",
    "  4295: ...\n",
    "}\n",
    "```\n",
    "where the numbers in semantic_eq_groups stand for the semantic equivalence class they belong to.\n",
    "\n",
    "The algo is written as pseudocode on page 15"
   ],
   "id": "ae14b87ee3d8ae27"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T20:19:29.580673Z",
     "start_time": "2024-06-13T20:19:29.562271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def bidirectional_entailment(question, answer1, answer2):\n",
    "    \"\"\"\n",
    "    Tests whether bidirectional entailment Question answer1 <=> Question answer2 holds\n",
    "    :return: True for bidirectional entailment, False otherwise\n",
    "    \"\"\"\n",
    "    true_answer = \"Answer: \" + answer1\n",
    "    question = \"Question: \" + question\n",
    "    generated_answer = \"Answer: \" + answer2\n",
    "\n",
    "    # First direction\n",
    "    premise = question + \" \" + generated_answer\n",
    "    hypothesis = question + \" \" + true_answer\n",
    "    input = tokenizer(premise, hypothesis, return_tensors=\"pt\")\n",
    "    output = model(input[\"input_ids\"].to(device))\n",
    "    prediction = torch.argmax(output[\"logits\"][0], dim=-1).item()  # 0: entail, 1: neutral, 2: contradiction\n",
    "\n",
    "    # Only if first direction entailment: look at second direction\n",
    "    if prediction == 0:\n",
    "        input = tokenizer(hypothesis, premise, return_tensors=\"pt\")\n",
    "        output = model(input[\"input_ids\"].to(device))\n",
    "        prediction = torch.argmax(output[\"logits\"][0], dim=-1).item()\n",
    "        if prediction == 0:\n",
    "            return True\n",
    "\n",
    "    return False"
   ],
   "id": "289786d2bf38112",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T20:20:59.605804Z",
     "start_time": "2024-06-13T20:20:59.565290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def bidirectional_entailment_clustering(question, answers):\n",
    "    # Save as list as order is needed (\"Use first sequence for each semantic-class\")\n",
    "    # Only save indices of answers\n",
    "    meanings = [[0]]\n",
    "\n",
    "    for m in range(1, len(answers)):\n",
    "        added = False\n",
    "        for equivalence_class_nr, equivalence_class in enumerate(meanings):\n",
    "            # Use first sequence for each semantic class\n",
    "            s_c = answers[equivalence_class[0]]\n",
    "            if bidirectional_entailment(question, s_c, answers[m]):\n",
    "                meanings[equivalence_class_nr].append(m)\n",
    "                added = True\n",
    "                break\n",
    "        if not added:\n",
    "            meanings.append([m])\n",
    "    \n",
    "    return meanings"
   ],
   "id": "2ca1481397ce8274",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T20:21:03.698112Z",
     "start_time": "2024-06-13T20:21:01.073236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = load_pickle_files(save_path)\n",
    "config_keys = [f\"temperature_{t}\" for t in config[\"temperatures\"]] + [f\"beam_{b}\" for b in config[\"n_beams\"]]"
   ],
   "id": "f5f7b97805bf93f9",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-06-13T20:21:03.826665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for group_nr, group in enumerate(data):\n",
    "    for question_id in tqdm(group):\n",
    "        for k in config_keys:\n",
    "            answers = group[question_id][k][\"answers\"]\n",
    "            question = group[question_id][\"question\"]\n",
    "            semantic_eq_groups = [-1] * len(answers)\n",
    "\n",
    "            semantic_clusters = bidirectional_entailment_clustering(question, answers)\n",
    "            for cluster_id, cluster in enumerate(semantic_clusters):\n",
    "                for index in cluster:\n",
    "                    semantic_eq_groups[index] = cluster_id\n",
    "          \n",
    "            group[question_id][k][\"semantic_eq_groups\"] = semantic_eq_groups\n",
    "            \n",
    "            print(answers)\n",
    "            print(semantic_eq_groups)\n",
    "    # Save result\n",
    "    #with open(os.path.join(save_path, f\"group{group_nr}.pkl\"), \"wb\") as f:\n",
    "    #    pickle.dump(group, f)\n",
    "    "
   ],
   "id": "16e443878c17325",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "83d1463062b02997"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
